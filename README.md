# üöÄ AWS Real-Time Product Recommendation Pipeline

![](./images/webapp.png)

## üìã Project Overview

This project implements a comprehensive serverless real-time product recommendation system using AWS services. The system combines streaming data processing with machine learning to provide personalized product recommendations to users in real-time.

### Key Capabilities

- **Real-Time Recommendations**: Generate personalized product suggestions using ML models
- **Streaming Data Pipeline**: Process user interactions in real-time via Kinesis
- **Serverless Architecture**: Fully managed AWS services with auto-scaling
- **Modern Web Interface**: Responsive web application for user interactions
- **ML Pipeline Automation**: Automated feature engineering and model training

## üèóÔ∏è Architecture

The system implements a modern serverless architecture combining real-time data processing with machine learning inference:

### Core Components

- **Web Application**: Modern responsive UI for user interaction and product recommendations
- **API Gateway**: RESTful endpoints for receiving user data and serving recommendations
- **Lambda Functions**: 
  - **Inference Lambda**: Fetches user features from DynamoDB and generates recommendations via SageMaker
  - **Data Processing**: Handles user data and forwards to Kinesis for analytics
- **SageMaker Endpoint**: Real-time XGBoost model inference for product recommendations
- **Kinesis Data Stream**: Real-time streaming of user interaction data
- **Firehose**: Batch delivery of stream data to S3 for analytics
- **DynamoDB**: Fast access to pre-computed user and product features
- **S3 Buckets**: Storage for raw data, processed features, and model artifacts
- **Step Functions**: Orchestrates the complete ML pipeline workflow
- **CloudFront**: Global content delivery network for the web application

### ML Pipeline Components

- **AWS Glue**: Automated feature engineering and data processing
- **SageMaker Training**: XGBoost model training with historical data
- **Model Registry**: Versioned model artifacts in S3
- **Feature Store**: DynamoDB tables with pre-computed user/product features

## üéØ Data Flow

### Real-Time Recommendation Flow

1. **User Interaction**: User submits data via the web application
2. **API Gateway**: Receives POST request and triggers inference Lambda
3. **Feature Retrieval**: Lambda fetches user features from DynamoDB
4. **ML Inference**: SageMaker endpoint generates probability scores for products
5. **Product Metadata**: Lambda enriches recommendations with product details
6. **Response**: Personalized recommendations returned to user (< 500ms)
7. **Analytics**: User data simultaneously streamed to Kinesis for analytics

### ML Pipeline Flow

1. **Data Ingestion**: Raw user interaction data stored in S3
2. **Feature Engineering**: Glue job processes data and creates user/product features
3. **Feature Storage**: Processed features stored in DynamoDB for real-time access
4. **Model Training**: SageMaker trains XGBoost model on engineered features
5. **Model Deployment**: Trained model deployed to SageMaker endpoint
6. **Pipeline Orchestration**: Step Functions coordinates the entire workflow

![](./images/stepfunctions.png)

## ü§ñ Machine Learning Model

### Model Architecture

- **Algorithm**: XGBoost Classifier for binary prediction (will user buy this product?)
- **Features**: User behavior patterns, product popularity metrics, interaction history
- **Training Data**: Historical shopping cart and order data
- **Prediction**: Probability scores for user-product combinations

### Feature Engineering

The system generates rich features including:
- **User Features**: Order frequency, product diversity, reorder patterns, time since last order
- **Product Features**: Popularity metrics, reorder rates, department/aisle information
- **Interaction Features**: User-product historical interactions and preferences

### Real-Time Inference

- **Sub-second Response**: < 500ms end-to-end recommendation generation
- **Scalable**: Auto-scaling SageMaker endpoints handle variable load
- **Personalized**: Individual recommendations based on user behavior patterns
- **Fallback**: Graceful handling when features or models are unavailable

## üéØ Features

### Web Application
- ‚úÖ **Modern Responsive UI**: Clean, mobile-friendly interface
- ‚úÖ **Real-Time Recommendations**: Instant product suggestions
- ‚úÖ **Form Validation**: Client-side input validation
- ‚úÖ **Error Handling**: Graceful error states and retry logic
- ‚úÖ **Cross-Browser Support**: Works across modern browsers

### Infrastructure
- ‚úÖ **Serverless Architecture**: No server management required
- ‚úÖ **Auto-Scaling**: Handles traffic spikes automatically
- ‚úÖ **Security**: IAM roles with least-privilege access
- ‚úÖ **Monitoring**: CloudWatch logs and metrics
- ‚úÖ **Global CDN**: CloudFront for worldwide performance

### ML Pipeline
- ‚úÖ **Automated Training**: Scheduled model retraining
- ‚úÖ **Feature Engineering**: Automated data processing with Glue
- ‚úÖ **Model Versioning**: Tracked model artifacts in S3
- ‚úÖ **A/B Testing Ready**: Infrastructure supports model experimentation
- ‚úÖ **Monitoring**: Model performance tracking and alerts

## üìÅ Project Structure

```
kinesis/
‚îú‚îÄ‚îÄ environments/dev/          # Terraform environment configuration
‚îÇ   ‚îú‚îÄ‚îÄ main.tf               # Main infrastructure definition
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf          # Environment variables
‚îÇ   ‚îî‚îÄ‚îÄ terraform.tfvars      # Environment-specific values
‚îú‚îÄ‚îÄ modules/                   # Reusable Terraform modules
‚îÇ   ‚îú‚îÄ‚îÄ api-gateway/          # API Gateway configuration
‚îÇ   ‚îú‚îÄ‚îÄ lambda/               # Lambda functions (inference + processing)
‚îÇ   ‚îú‚îÄ‚îÄ sagemaker/           # ML training and endpoints
‚îÇ   ‚îú‚îÄ‚îÄ step-functions/       # ML pipeline orchestration
‚îÇ   ‚îú‚îÄ‚îÄ glue-job/            # Feature engineering
‚îÇ   ‚îú‚îÄ‚îÄ dynamodb/            # Feature storage
‚îÇ   ‚îú‚îÄ‚îÄ kinesis/             # Data streaming
‚îÇ   ‚îú‚îÄ‚îÄ firehose/            # Data delivery
‚îÇ   ‚îú‚îÄ‚îÄ s3/                  # Data storage
‚îÇ   ‚îú‚îÄ‚îÄ s3-webapp/           # Web app hosting
‚îÇ   ‚îú‚îÄ‚îÄ cloudfront/          # CDN configuration
‚îÇ   ‚îî‚îÄ‚îÄ vpc/                 # Network infrastructure
‚îú‚îÄ‚îÄ webapp/                   # Web application
‚îÇ   ‚îú‚îÄ‚îÄ index.html           # Main application
‚îÇ   ‚îú‚îÄ‚îÄ js/app.js            # Application logic
‚îÇ   ‚îú‚îÄ‚îÄ css/styles.css       # Styling
‚îÇ   ‚îî‚îÄ‚îÄ server.js            # Local development server
‚îú‚îÄ‚îÄ other_scripts/           # Utility scripts
‚îú‚îÄ‚îÄ Makefile                 # Build and deployment automation
‚îú‚îÄ‚îÄ pipeline.md              # Technical pipeline documentation
‚îî‚îÄ‚îÄ README.md                # This file
```

## üöÄ Quick Start

### Prerequisites

- AWS CLI configured with appropriate permissions
- Terraform >= 1.0
- Node.js (for local web app development)
- Make utility

### Deployment Steps

1. **Initialize Terraform**
   ```bash
   cd environments/dev
   terraform init
   ```

2. **Deploy Core Infrastructure**
   ```bash
   terraform apply -target=module.s3 -target=module.dynamodb -target=module.glue_job
   ```

3. **Run ML Pipeline**
   ```bash
   terraform apply -target=module.step_functions
   make execute-step-function
   ```
   
   This automatically:
   - Processes data and engineers features
   - Trains the XGBoost recommendation model
   - Deploys the SageMaker inference endpoint

4. **Deploy Application Services**
   ```bash
   cd modules/lambda && ./create_package.sh
   cd ../../environments/dev
   terraform apply -target=module.lambda -target=module.api_gateway
   ```

5. **Deploy Web Application**
   ```bash
   terraform apply -target=module.kinesis -target=module.firehose -target=module.s3_webapp -target=module.cloudfront
   make update-api-url
   make deploy-webapp
   ```

### Access Your Application

After deployment, access your application via:
- **CloudFront URL**: `https://your-cloudfront-domain.cloudfront.net`
- **API Gateway**: Check Terraform outputs for the API endpoint

## üß™ Testing

### Test Recommendations

```bash
curl -X POST https://your-api-gateway-url/submit \
  -H "Content-Type: application/json" \
  -d '{"user_id": "123", "event": "get_recommendations"}'
```

### Expected Response

```json
{
  "message": "Recommendations generated successfully",
  "recommendations": [
    {
      "product_id": "123",
      "probability": 0.85,
      "product_name": "Organic Bananas",
      "department": "produce",
      "aisle": "fresh fruits"
    }
  ]
}
```

## üîß Configuration

### Environment Variables

Key environment variables in `terraform.tfvars`:

```hcl
# Infrastructure
aws_region = "ap-southeast-2"
environment = "dev"

# ML Configuration  
sagemaker_instance_type = "ml.t2.medium"
model_name = "xgboost-recommender"

# Application
cors_allowed_origins = ["*"]
```

## üìä Monitoring

### CloudWatch Dashboards

- **Lambda Performance**: Function duration, errors, invocations
- **SageMaker Metrics**: Endpoint utilization, inference latency
- **Kinesis Metrics**: Stream throughput, shard utilization
- **API Gateway**: Request rates, latency, error rates

### Logging

- **Lambda Logs**: `/aws/lambda/{function-name}`
- **SageMaker Logs**: `/aws/sagemaker/TrainingJobs`
- **Step Functions**: Execution history and state transitions
- **API Gateway**: Request/response logging (configurable)

## üõ†Ô∏è Development

### Local Development

1. **Run Web App Locally**
   ```bash
   cd webapp
   npm install
   node server.js
   ```

2. **Test Lambda Functions**
   ```bash
   cd modules/lambda
   python -m pytest tests/
   ```

3. **Validate Terraform**
   ```bash
   terraform validate
   terraform plan
   ```

### Code Organization

- **Infrastructure as Code**: All AWS resources defined in Terraform
- **Modular Design**: Reusable Terraform modules for each service
- **Environment Separation**: Dev/staging/prod configurations
- **Version Control**: Git-based workflow with feature branches

## üö® Troubleshooting

### Common Issues

1. **SageMaker Endpoint Not Ready**
   - Check Step Functions execution status
   - Verify endpoint is "InService" in SageMaker console

2. **DynamoDB Empty Features**
   - Ensure Glue job completed successfully
   - Check CloudWatch logs for processing errors

3. **Lambda Timeout**
   - Increase timeout in Terraform configuration
   - Optimize feature queries and model inference

4. **API Gateway CORS Issues**
   - Verify CORS headers in Lambda response
   - Check browser developer console for specific errors

### Performance Optimization

- **DynamoDB**: Use composite keys for efficient queries
- **Lambda**: Optimize memory allocation based on usage patterns
- **SageMaker**: Choose appropriate instance types for load
- **CloudFront**: Configure caching for static assets

## üîê Security

### Best Practices Implemented

- ‚úÖ **IAM Least Privilege**: Minimal required permissions per service
- ‚úÖ **VPC Security Groups**: Network-level access controls
- ‚úÖ **Encryption**: Data encrypted at rest and in transit
- ‚úÖ **API Authentication**: Ready for OAuth/JWT integration
- ‚úÖ **Resource Isolation**: Environment-specific resource naming

### Security Considerations

- API Gateway endpoints are currently public (add authentication for production)
- Consider VPC endpoints for enhanced security
- Enable AWS CloudTrail for audit logging
- Implement API rate limiting for production workloads

## üìà Cost Optimization

### Cost-Saving Features

- **Serverless**: Pay only for actual usage
- **Auto-Scaling**: Resources scale down during low usage
- **Spot Instances**: Optional for batch training jobs
- **Data Lifecycle**: Automated S3 lifecycle policies

### Estimated Monthly Costs (Light Usage)

- Lambda: $5-15
- SageMaker Endpoint: $30-50
- DynamoDB: $5-10
- Kinesis: $10-20
- S3 + CloudFront: $5-15
- **Total**: ~$55-110/month

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìÑ License

This project is licensed under the MIT License. See `LICENSE` file for details.

## üôè Acknowledgments

- AWS for providing the serverless infrastructure platform
- XGBoost team for the machine learning algorithm
- Terraform for infrastructure as code capabilities
