from pyspark.sql import functions as F
from pyspark.sql.functions import when, col
from pyspark.sql.window import Window
from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame
from awsglue.job import Job
from pyspark.context import SparkContext
import os
import sys
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.functions import vector_to_array
import boto3
import joblib


class FeatureEngineering:
    def __init__(self, database=None, output_bucket=None):
        """Initialize Spark context and configurations."""
        try:
            print(" Initializing Spark context...")
            self.sc = SparkContext.getOrCreate()
            self.glueContext = GlueContext(self.sc)
            self.spark = self.glueContext.spark_session
            self.job = Job(self.glueContext)
            self.database = database or '${database}'
            self.output_bucket = output_bucket or '${output_bucket}'
            print(f"âœ… Spark context initialized. Database: {self.database}, Output: {self.output_bucket}")
        except Exception as e:
            print(f"âŒ Error initializing Spark context: {e}")
            sys.exit(1)
        
    def load_data(self):
        """Load all required tables from Glue Catalog."""
        try:
                # List available databases and tables for debugging

            print("ğŸ“Š Loading data from Glue Catalog...")
            self.products_df = self._load_table('products')
            print(f"âœ… Loaded products: {self.products_df.count()} rows")
            
            self.orders_df = self._load_table('orders')
            print(f"âœ… Loaded orders: {self.orders_df.count()} rows")
            
            self.aisles_df = self._load_table('aisles')
            print(f"âœ… Loaded aisles: {self.aisles_df.count()} rows")
            
            self.departments_df = self._load_table('departments')
            print(f"âœ… Loaded departments: {self.departments_df.count()} rows")
            
            self.order_products__prior_df = self._load_table('order_products__prior')
            self.order_products__train_df = self._load_table('order_products__train')
            
            self.order_products_df = self.order_products__prior_df.unionByName(self.order_products__train_df) \
                                        .withColumn(
                                            "reordered_int",
                                            when(F.lower(col("reordered").cast("string")) == "true", 1)
                                            .when(F.lower(col("reordered").cast("string")) == "false", 0)
                                            .otherwise(None)
                                        ).drop("reordered") \
                                        .withColumnRenamed("reordered_int", "reordered")

            # self.order_products_df = self._load_table('order_products')
            print(f"âœ… Loaded order_products: {self.order_products_df.count()} rows")
            
            # Clean days_since_prior_order
            self.orders_df = self.orders_df.withColumnRenamed("days_since_prior", "days_since_prior_order").withColumn(
                "days_since_prior_order",
                when(col("days_since_prior_order").isNull(), 0).otherwise(col("days_since_prior_order"))
            )
            
            # Filter order sets
            self.orders_prior_df = self.orders_df.filter(col("eval_set") == "prior")
            self.train_orders_df = self.orders_df.filter(col("eval_set") == "train").select("user_id", "order_id")
            self.test_orders_df = self.orders_df.filter(col("eval_set") == "test").select("user_id", "order_id")
            
            # Join orders with products
            self.order_products_prior = self.orders_prior_df.join(self.order_products_df, "order_id") \
                                      .select("user_id", "order_id", "order_number", 
                                              "days_since_prior_order", "product_id", 
                                              "add_to_cart_order", "reordered") \
                                        
               
            print(f"âœ… Created order_products_prior: {self.order_products_prior.count()} rows")
            
        except Exception as e:
            print(f"âŒ Error loading data: {e}")
            raise
    
    def _load_table(self, table):
        """Helper to load a table from Glue Catalog."""
        try:
            print(f"ğŸ“– Loading table: {table}")
            df = self.glueContext.create_dynamic_frame.from_catalog(
                database=self.database, 
                table_name=table
            ).toDF()
            print(f"âœ… Successfully loaded {table}")
            return df
        except Exception as e:
            print(f"âŒ Error loading table {table}: {e}")
            raise
    
    def _save_parquet(self, df, path_suffix):
        """Save DataFrame as Parquet."""
        try:
            path = f"s3://{self.output_bucket}/features/{path_suffix}"
            print(f"ğŸ’¾ Saving Parquet: {path}")
            df.write.mode("overwrite").parquet(path)
            print(f"âœ… Saved Parquet: {path}")
        except Exception as e:
            print(f"âŒ Error saving Parquet {path}: {e}")
            raise

 

    def _save_to_dynamodb(self, df, table_name):
        """Save DataFrame to DynamoDB."""
        try:
            print(f" Saving to DynamoDB: {table_name}")
            dyf = DynamicFrame.fromDF(df, self.glueContext, table_name)
            self.glueContext.write_dynamic_frame.from_options(
                frame=dyf,
                connection_type="dynamodb",
                connection_options={
                    "dynamodb.output.tableName": table_name,
                    "dynamodb.throughput.write.percent": "0.5"
                }
            )
            print(f"âœ… Saved to DynamoDB: {table_name}")
        except Exception as e:
            print(f"âŒ Error saving to DynamoDB {table_name}: {e}")
            raise
    
    def _apply_standard_scaler(self, df, id_cols):
        """
        Fit a StandardScaler on the given DataFrame (excluding id_cols), 
        and return the scaled DataFrame and the fitted scaler model.
        """
        feature_cols = [col for col in df.columns if col not in id_cols]
        assembler = VectorAssembler(inputCols=feature_cols, outputCol="features_vec")
        df_vec = assembler.transform(df)
        scaler = StandardScaler(inputCol="features_vec", outputCol="features_scaled", withMean=True, withStd=True)
        scaler_model = scaler.fit(df_vec)
        df_scaled = scaler_model.transform(df_vec)
        df_scaled = df_scaled.withColumn("features_array", vector_to_array("features_scaled"))
        for i, col_name in enumerate(feature_cols):
            df_scaled = df_scaled.withColumn(col_name + "_scaled", df_scaled.features_array[i])
        scaled_col_names = [col + "_scaled" for col in feature_cols]
        return df_scaled.select(*id_cols, *scaled_col_names), scaler_model

    def _transform_standard_scaler(self, df, id_cols, scaler_model):
        """
        Transform a DataFrame using a "fitted StandardScaler" model (excluding id_cols).
        """
        feature_cols = [col for col in df.columns if col not in id_cols]
        assembler = VectorAssembler(inputCols=feature_cols, outputCol="features_vec")
        df_vec = assembler.transform(df)
        df_scaled = scaler_model.transform(df_vec)
        df_scaled = df_scaled.withColumn("features_array", vector_to_array("features_scaled"))
        for i, col_name in enumerate(feature_cols):
            df_scaled = df_scaled.withColumn(col_name + "_scaled", df_scaled.features_array[i])
        scaled_col_names = [col + "_scaled" for col in feature_cols]
        return df_scaled.select(*id_cols, *scaled_col_names)

    # In your Glue job (features.py)
    def save_scaler_parameters(self, scaler_model, output_bucket):
        """Extract PySpark scaler parameters and save as sklearn-compatible format"""
        try:
            import numpy as np
            from sklearn.preprocessing import StandardScaler
            
            # Extract parameters from PySpark scaler
            mean_values = scaler_model.mean.toArray()
            std_values = scaler_model.std.toArray()
            
            # Create sklearn StandardScaler with same parameters
            sklearn_scaler = StandardScaler()
            sklearn_scaler.mean_ = mean_values
            sklearn_scaler.scale_ = 1.0 / std_values  # sklearn uses scale (1/std)
            sklearn_scaler.var_ = std_values ** 2
            sklearn_scaler.n_features_in_ = len(mean_values)
            sklearn_scaler.feature_names_in_ = None  # Optional
            
            # Save sklearn-compatible scaler
            joblib.dump(sklearn_scaler, "sklearn_scaler.pkl")
            boto3.client('s3').upload_file("sklearn_scaler.pkl", output_bucket, "scale_models/sklearn_scaler.pkl")
            print("âœ… Saved sklearn-compatible scaler to S3")
            
        except Exception as e:
            print(f"âŒ Error saving sklearn scaler: {e}")
        
    def create_product_metadata(self):
        """Create and save product metadata."""
        try:
            print("ğŸ­ Creating product metadata...")
            products = self.products_df.join(self.aisles_df, "aisle_id") \
                                      .join(self.departments_df, "department_id")                        
            
            self._save_to_dynamodb(products, "products")
            print("âœ… Saved product metadata to DynamoDB: products")
            return products
        except Exception as e:
            print(f"âŒ Error creating product metadata: {e}")
            raise
    
    def create_user_features(self):
        """Create user-level features."""
        try:
            print("ğŸ­ Creating user features...")
            # User order patterns
            user_features_1 = self.orders_prior_df.groupBy("user_id").agg(
                F.max("order_number").alias("user_orders"),
                F.sum("days_since_prior_order").alias("user_periods"),
                F.mean("days_since_prior_order").alias("user_mean_days_since_prior")
            )
            
            
            # User product patterns
            order_products_prior_flag = self.order_products_prior.withColumn(
                "reordered_flag", when(col("reordered") == 1, 1).otherwise(0)
            ).withColumn(
                "repeat_order_flag", when(col("order_number") > 1, 1).otherwise(None)
            )
            
            user_features_2 = order_products_prior_flag.groupBy("user_id").agg(
                F.count("product_id").alias("user_products"),
                F.countDistinct("product_id").alias("user_distinct_products"),
                (F.sum("reordered_flag") / F.count("repeat_order_flag")).alias("user_reorder_ratio")
            )

            user_features = user_features_1.join(user_features_2, "user_id")

            # self._save_parquet(user_features, "user_features")
            # print("âœ… Saved user features to S3: user_features")
            # self._save_to_dynamodb(user_features.filter(col("user_id") < 10000), "user_features")
            # print("âœ… Saved user features to DynamoDB: user_features")
            
            return user_features

        except Exception as e:
            print(f"âŒ Error creating user features: {e}")
            raise

    
    def create_user_product_features(self):
        """Create user-product interaction features."""
        try:
            print("ğŸ­ Creating user-product interaction features...")
            # Exclude train orders
            order_products_prior_excl_train = self.order_products_prior.join(
                self.train_orders_df, ["user_id", "order_id"], "left_anti"
            )
            
            up_features = order_products_prior_excl_train.groupBy("user_id", "product_id").agg(
                F.count("*").alias("up_order_count"),
                F.min("order_number").alias("up_first_order_number"),
                F.max("order_number").alias("up_last_order_number"),
                F.avg("add_to_cart_order").alias("up_avg_cart_position")
            )
            self._save_parquet(up_features, "up_features")
            print("âœ… Saved user-product interaction features to S3: up_features")
            return up_features
            
        except Exception as e:
            print(f"âŒ Error creating user-product interaction features: {e}")
            raise
    
    def create_product_features(self):
        """Create product-level features."""
        try:
            print("ğŸ­ Creating product-level features...")
            product_seq_window = Window.partitionBy("user_id", "product_id").orderBy("order_number")
            product_seq_df = self.order_products_prior.withColumn(
                "product_seq_time", F.row_number().over(product_seq_window)
            )
            
            prd_features = product_seq_df.groupBy("product_id").agg(
                F.count("*").alias("prod_orders"),
                F.sum(when(col("reordered") == 1, 1).otherwise(0)).alias("prod_reorders"),
                F.sum(when(col("product_seq_time") == 1, 1).otherwise(0)).alias("prod_first_orders"),
                F.sum(when(col("product_seq_time") == 2, 1).otherwise(0)).alias("prod_second_orders")
            )
            # self._save_parquet(prd_features, "prd_features")
            # print("âœ… Saved product-level features to S3: prd_features")
            # self._save_to_dynamodb(prd_features, "product_features")
            # print("âœ… Saved product-level features to DynamoDB: product_features")

            return prd_features
        except Exception as e:
            print(f"âŒ Error creating product-level features: {e}")
            raise

    
    
    def create_training_data(self, user_features, prd_features):
        """Create and save training dataset."""
        try:
            print("ğŸ“Š Creating training dataset...")
            train_labels = self.order_products_df.join(self.train_orders_df, "order_id") \
                                               .select("user_id", "product_id", "reordered")
            
            train_df = train_labels.join(user_features, "user_id", "left") \
                                     .join(prd_features, "product_id", "left") \
                                     .fillna(0) \
                                     .select('product_id', 'user_id', 'reordered', 'user_orders', 'user_periods',
                                        'user_mean_days_since_prior', 'user_products', 'user_distinct_products',
                                        'user_reorder_ratio', 'prod_orders', 'prod_reorders',
                                        'prod_first_orders', 'prod_second_orders')

            # self._save_parquet(train_df, "train")
            # print("âœ… Saved training dataset to S3: train")
            
            id_cols = ['product_id', 'user_id', 'reordered']
            
            # Fit scaler on train
            train_df_scaled, scaler_model = self._apply_standard_scaler(train_df, id_cols)
            # Save scaler_model in sklearn format to S3 
            self.save_scaler_parameters(scaler_model, self.output_bucket)
            
            # Use label "reordered" and scaled features for training
            train_df_scaled = train_df_scaled.select('reordered', 'user_orders_scaled', 'user_periods_scaled',
                                        'user_mean_days_since_prior_scaled', 'user_products_scaled', 'user_distinct_products_scaled',
                                        'user_reorder_ratio_scaled', 'prod_orders_scaled', 'prod_reorders_scaled',
                                        'prod_first_orders_scaled', 'prod_second_orders_scaled')
            
            self._save_parquet(train_df_scaled, "train_scaled")
            print("âœ… Saved training dataset to S3: train_scaled")

            self._scaler_model = scaler_model
            self._id_cols = id_cols
            return train_df

        except Exception as e:
            print(f"âŒ Error creating training dataset: {e}")
            raise
    
    def create_test_data(self, user_features, prd_features):
        """Create and save test dataset."""
        try:
            print("ğŸ“Š Creating test dataset...")
            candidates = self.order_products_prior.select("user_id", "product_id").distinct()
            test_candidates = self.test_orders_df.join(candidates, "user_id")
            
            test_features_df = test_candidates.join(user_features, "user_id", "left") \
                                             .join(prd_features, "product_id", "left") \
                                             .fillna(0) \
                                             .select('product_id', 'user_id', 'user_orders', 'user_periods',
                                        'user_mean_days_since_prior', 'user_products', 'user_distinct_products',
                                        'user_reorder_ratio', 'prod_orders', 'prod_reorders',
                                        'prod_first_orders', 'prod_second_orders')
            # Use scaler from train
            id_cols = ['product_id', 'user_id']

            
            test_df_scaled = self._transform_standard_scaler(test_features_df, id_cols, self._scaler_model)
            test_df_scaled = test_df_scaled.select('user_orders_scaled', 'user_periods_scaled',
                                        'user_mean_days_since_prior_scaled', 'user_products_scaled', 'user_distinct_products_scaled',
                                        'user_reorder_ratio_scaled', 'prod_orders_scaled', 'prod_reorders_scaled',
                                        'prod_first_orders_scaled', 'prod_second_orders_scaled')

            self._save_parquet(test_features_df, "test")
            print("âœ… Saved test dataset to S3: test")
            self._save_parquet(test_df_scaled, "test_scaled")
            print("âœ… Saved test dataset to S3: test_scaled")
            return test_features_df
        except Exception as e:
            print(f"âŒ Error creating test dataset: {e}")
            raise

    def prepare_dynamodb_feature_table(self, user_features, prd_features):
        """Join user-product pairs with features and store in DynamoDB for real-time inference."""
        try:
            print("ï¸ Creating DynamoDB lookup table...")
            user_product_df = self.order_products_prior.select("user_id", "product_id").distinct()

            # Join features
            feature_df = user_product_df.join(user_features, on="user_id", how="left") \
                                        .join(prd_features, on="product_id", how="left") \
                                        .fillna(0) \
                                        .select('product_id', 'user_id', 'user_orders', 'user_periods',
                                            'user_mean_days_since_prior', 'user_products', 'user_distinct_products',
                                            'user_reorder_ratio', 'prod_orders', 'prod_reorders',
                                            'prod_first_orders', 'prod_second_orders')

            # Use the scaler fitted on training data
            id_cols = ['user_id', 'product_id']
            feature_df_scaled = self._transform_standard_scaler(feature_df, id_cols, self._scaler_model)
            feature_df_scaled = feature_df_scaled.select("user_id", "product_id", 'user_orders_scaled', 'user_periods_scaled',
                                        'user_mean_days_since_prior_scaled', 'user_products_scaled', 'user_distinct_products_scaled',
                                        'user_reorder_ratio_scaled', 'prod_orders_scaled', 'prod_reorders_scaled',
                                        'prod_first_orders_scaled', 'prod_second_orders_scaled')
            
            # self._save_parquet(feature_df_scaled, "user_product_features")
            # print("âœ… Saved prior features to S3: user_product_features")

            # Save to DynamoDB (limit for budget )
            self._save_to_dynamodb(feature_df_scaled.filter(col("user_id") < 5000), "user_product_features")
            print("âœ… Saved prior user-product feature table to DynamoDB: user_product_features")

            return feature_df
        except Exception as e:
            print(f"âŒ Error preparing DynamoDB lookup table: {e}")
            raise
    
    def run_pipeline(self):
        """Execute the full feature engineering pipeline."""
        try:
            print("ğŸ¯ Starting feature engineering pipeline...")
            
            self.load_data()

            # self._save_parquet(self.order_products_prior, "order_products_prior")
            
            print("ğŸ­ Creating features...")
            # Create features
            # self.create_product_metadata()
            user_features = self.create_user_features()
            # self.create_user_product_features()
            prd_features = self.create_product_features()
            
            print("ğŸ“Š Creating datasets...")
            # Create datasets
            self.create_training_data(user_features, prd_features)
            # self.create_test_data(user_features, prd_features)


            print("ï¸ Creating DynamoDB lookup table...")
            # Create real-time lookup table in DynamoDB
            self.prepare_dynamodb_feature_table(user_features, prd_features)

            print("ğŸ‰ Feature engineering pipeline completed successfully!")
            
        except Exception as e:
            print(f"âŒ Pipeline failed: {e}")
            raise

if __name__ == "__main__":
    try:
        pipeline = FeatureEngineering()
        pipeline.run_pipeline()
    except Exception as e:
        print(f"âŒ Job failed: {e}")
        sys.exit(1)
